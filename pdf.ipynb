{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"2.1 CNN의 구조 및 학습\n",
    "CNN은 그림 1과 같이 크게 convolution-pooling layer\n",
    "와 fully connected layer의 두 부분으로 구성되어 있\n",
    "다. 전자는 입력된 이미지로부터 계층적 구조의\n",
    "feature를 추출하는 역할을, 후자는 추출된 feature를\n",
    "입력받아 타겟 클래스로 분류하는 역할을 담당한다.\n",
    "CNN layer는 아래와 같은 두 가지 특징을 통해 이\n",
    "미지 데이터의 특성을 반영하는 동시에 모델의 복잡\n",
    "도를 크게 단순화시킨다.\n",
    "1) Local connectivity: 가장 일반적인 형태인 fully\n",
    "connected layer와 달리, 해당 convolution 필터의 크기\n",
    "인 NxN window 내의 인접한 뉴런들에만 연결이 되\n",
    "어있다. 이는 인접한 픽셀들끼리는 상관관계가 높지\n",
    "만 멀리 떨어진 픽셀들은 그렇지 않은 이미지의 특성\n",
    "특집원고\n",
    "인천대학교 | IP:117.16.225.*** | Accessed 2021/01/11 23:04(KST)\n",
    "26 특집원고 다양한 딥러닝 알고리즘과 활용\n",
    "(a) (b) (c)\n",
    "그림 2 자동차 이미지로 학습시킨 CNN의 (a) 하위 layer, (b) 중위 layer, (c) 상위 layer에서 학습한 feature [5]\n",
    "(a) (b) (c)\n",
    "그림 3 CNN을 이용한 네이버의 서비스. (a) N드라이브 사진 검색 (b) 지식iN 사진 질문 기반 디렉토리 추천 (c) 이미지 기반\n",
    "라인 신규 스티커 추천\n",
    "(locality)을 반영한 것이다.\n",
    "2) Shared weights: convolution 필터들은 적용되는\n",
    "위치가 달라도 같은 weight값을 공유한다. 이는 픽셀\n",
    "값의 통계적 특성이 이미지 상의 좌표와 무관하다는\n",
    "이미지의 특성(stationarity)을 반영한 것이다\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_preprocessing.text import Tokenizer\n",
    "t = Tokenizer()\n",
    "t.fit_on_texts([text])\n",
    "encoded = t.texts_to_sequences([text])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 집합의 크기: 155\n",
      "{'a': 1, 'b': 2, 'c': 3, '2': 4, '그림': 5, 'convolution': 6, 'layer': 7, '1': 8, 'cnn의': 9, '크게': 10, 'fully': 11, 'connected': 12, '두': 13, 'feature를': 14, '역할을': 15, '같은': 16, '인접한': 17, '이는': 18, '이미지의': 19, '특성': 20, '특집원고': 21, '사진': 22, '기반': 23, '추천': 24, '이미지': 25, '을': 26, '반영한': 27, '것이다': 28, '구조': 29, '및': 30, '학습': 31, 'cnn은': 32, '1과': 33, '같이': 34, 'pooling': 35, '와': 36, 'layer의': 37, '부분으로': 38, '구성되어': 39, '있': 40, '다': 41, '전자는': 42, '입력된': 43, '이미지로부터': 44, '계층적': 45, '구조의': 46, '추출하는': 47, '후자는': 48, '추출된': 49, '입력받아': 50, '타겟': 51, '클래스로': 52, '분류하는': 53, '담당한다': 54, 'cnn': 55, 'layer는': 56, '아래와': 57, '가지': 58, '특징을': 59, '통해': 60, '이': 61, '미지': 62, '데이터의': 63, '특성을': 64, '반영하는': 65, '동시에': 66, '모델의': 67, '복잡': 68, '도를': 69, '단순화시킨다': 70, 'local': 71, 'connectivity': 72, '가장': 73, '일반적인': 74, '형태인': 75, 'layer와': 76, '달리': 77, '해당': 78, '필터의': 79, '크기': 80, '인': 81, 'nxn': 82, 'window': 83, '내의': 84, '뉴런들에만': 85, '연결이': 86, '되': 87, '어있다': 88, '픽셀들끼리는': 89, '상관관계가': 90, '높지': 91, '만': 92, '멀리': 93, '떨어진': 94, '픽셀들은': 95, '그렇지': 96, '않은': 97, '인천대학교': 98, 'ip': 99, '117': 100, '16': 101, '225': 102, 'accessed': 103, '2021': 104, '01': 105, '11': 106, '23': 107, '04': 108, 'kst': 109, '26': 110, '다양한': 111, '딥러닝': 112, '알고리즘과': 113, '활용': 114, '자동차': 115, '이미지로': 116, '학습시킨': 117, '하위': 118, '중위': 119, '상위': 120, 'layer에서': 121, '학습한': 122, 'feature': 123, '5': 124, '3': 125, 'cnn을': 126, '이용한': 127, '네이버의': 128, '서비스': 129, 'n드라이브': 130, '검색': 131, '지식in': 132, '질문': 133, '디렉토리': 134, '라인': 135, '신규': 136, '스티커': 137, 'locality': 138, 'shared': 139, 'weights': 140, '필터들은': 141, '적용되는': 142, '위치가': 143, '달라도': 144, 'weight값을': 145, '공유한다': 146, '픽셀': 147, '값의': 148, '통계적': 149, '특성이': 150, '상의': 151, '좌표와': 152, '무관하다는': 153, 'stationarity': 154}\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(t.word_index)+1\n",
    "\n",
    "print('단어 집합의 크기: %d' % vocab_size)\n",
    "\n",
    "print(t.word_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 데이터의 개수: 165\n",
      "[[4, 8], [4, 8, 9], [4, 8, 9, 29], [4, 8, 9, 29, 30], [4, 8, 9, 29, 30, 31], [32, 5], [32, 5, 33], [32, 5, 33, 34], [32, 5, 33, 34, 10], [32, 5, 33, 34, 10, 6], [32, 5, 33, 34, 10, 6, 35], [32, 5, 33, 34, 10, 6, 35, 7], [36, 11], [36, 11, 12], [36, 11, 12, 37], [36, 11, 12, 37, 13], [36, 11, 12, 37, 13, 38], [36, 11, 12, 37, 13, 38, 39], [36, 11, 12, 37, 13, 38, 39, 40], [41, 42], [41, 42, 43], [41, 42, 43, 44], [41, 42, 43, 44, 45], [41, 42, 43, 44, 45, 46], [14, 47], [14, 47, 15], [14, 47, 15, 48], [14, 47, 15, 48, 49], [14, 47, 15, 48, 49, 14], [50, 51], [50, 51, 52], [50, 51, 52, 53], [50, 51, 52, 53, 15], [50, 51, 52, 53, 15, 54], [55, 56], [55, 56, 57], [55, 56, 57, 16], [55, 56, 57, 16, 13], [55, 56, 57, 16, 13, 58], [55, 56, 57, 16, 13, 58, 59], [55, 56, 57, 16, 13, 58, 59, 60], [55, 56, 57, 16, 13, 58, 59, 60, 61], [62, 63], [62, 63, 64], [62, 63, 64, 65], [62, 63, 64, 65, 66], [62, 63, 64, 65, 66, 67], [62, 63, 64, 65, 66, 67, 68], [69, 10], [69, 10, 70], [8, 71], [8, 71, 72], [8, 71, 72, 73], [8, 71, 72, 73, 74], [8, 71, 72, 73, 74, 75], [8, 71, 72, 73, 74, 75, 11], [12, 76], [12, 76, 77], [12, 76, 77, 78], [12, 76, 77, 78, 6], [12, 76, 77, 78, 6, 79], [12, 76, 77, 78, 6, 79, 80], [81, 82], [81, 82, 83], [81, 82, 83, 84], [81, 82, 83, 84, 17], [81, 82, 83, 84, 17, 85], [81, 82, 83, 84, 17, 85, 86], [81, 82, 83, 84, 17, 85, 86, 87], [88, 18], [88, 18, 17], [88, 18, 17, 89], [88, 18, 17, 89, 90], [88, 18, 17, 89, 90, 91], [92, 93], [92, 93, 94], [92, 93, 94, 95], [92, 93, 94, 95, 96], [92, 93, 94, 95, 96, 97], [92, 93, 94, 95, 96, 97, 19], [92, 93, 94, 95, 96, 97, 19, 20], [98, 99], [98, 99, 100], [98, 99, 100, 101], [98, 99, 100, 101, 102], [98, 99, 100, 101, 102, 103], [98, 99, 100, 101, 102, 103, 104], [98, 99, 100, 101, 102, 103, 104, 105], [98, 99, 100, 101, 102, 103, 104, 105, 106], [98, 99, 100, 101, 102, 103, 104, 105, 106, 107], [98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108], [98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109], [110, 21], [110, 21, 111], [110, 21, 111, 112], [110, 21, 111, 112, 113], [110, 21, 111, 112, 113, 114], [1, 2], [1, 2, 3], [5, 4], [5, 4, 115], [5, 4, 115, 116], [5, 4, 115, 116, 117], [5, 4, 115, 116, 117, 9], [5, 4, 115, 116, 117, 9, 1], [5, 4, 115, 116, 117, 9, 1, 118], [5, 4, 115, 116, 117, 9, 1, 118, 7], [5, 4, 115, 116, 117, 9, 1, 118, 7, 2], [5, 4, 115, 116, 117, 9, 1, 118, 7, 2, 119], [5, 4, 115, 116, 117, 9, 1, 118, 7, 2, 119, 7], [5, 4, 115, 116, 117, 9, 1, 118, 7, 2, 119, 7, 3], [5, 4, 115, 116, 117, 9, 1, 118, 7, 2, 119, 7, 3, 120], [5, 4, 115, 116, 117, 9, 1, 118, 7, 2, 119, 7, 3, 120, 121], [5, 4, 115, 116, 117, 9, 1, 118, 7, 2, 119, 7, 3, 120, 121, 122], [5, 4, 115, 116, 117, 9, 1, 118, 7, 2, 119, 7, 3, 120, 121, 122, 123], [5, 4, 115, 116, 117, 9, 1, 118, 7, 2, 119, 7, 3, 120, 121, 122, 123, 124], [1, 2], [1, 2, 3], [5, 125], [5, 125, 126], [5, 125, 126, 127], [5, 125, 126, 127, 128], [5, 125, 126, 127, 128, 129], [5, 125, 126, 127, 128, 129, 1], [5, 125, 126, 127, 128, 129, 1, 130], [5, 125, 126, 127, 128, 129, 1, 130, 22], [5, 125, 126, 127, 128, 129, 1, 130, 22, 131], [5, 125, 126, 127, 128, 129, 1, 130, 22, 131, 2], [5, 125, 126, 127, 128, 129, 1, 130, 22, 131, 2, 132], [5, 125, 126, 127, 128, 129, 1, 130, 22, 131, 2, 132, 22], [5, 125, 126, 127, 128, 129, 1, 130, 22, 131, 2, 132, 22, 133], [5, 125, 126, 127, 128, 129, 1, 130, 22, 131, 2, 132, 22, 133, 23], [5, 125, 126, 127, 128, 129, 1, 130, 22, 131, 2, 132, 22, 133, 23, 134], [5, 125, 126, 127, 128, 129, 1, 130, 22, 131, 2, 132, 22, 133, 23, 134, 24], [5, 125, 126, 127, 128, 129, 1, 130, 22, 131, 2, 132, 22, 133, 23, 134, 24, 3], [5, 125, 126, 127, 128, 129, 1, 130, 22, 131, 2, 132, 22, 133, 23, 134, 24, 3, 25], [5, 125, 126, 127, 128, 129, 1, 130, 22, 131, 2, 132, 22, 133, 23, 134, 24, 3, 25, 23], [135, 136], [135, 136, 137], [135, 136, 137, 24], [138, 26], [138, 26, 27], [138, 26, 27, 28], [4, 139], [4, 139, 140], [4, 139, 140, 6], [4, 139, 140, 6, 141], [4, 139, 140, 6, 141, 142], [143, 144], [143, 144, 16], [143, 144, 16, 145], [143, 144, 16, 145, 146], [143, 144, 16, 145, 146, 18], [143, 144, 16, 145, 146, 18, 147], [148, 149], [148, 149, 150], [148, 149, 150, 25], [148, 149, 150, 25, 151], [148, 149, 150, 25, 151, 152], [148, 149, 150, 25, 151, 152, 153], [19, 20], [19, 20, 154], [19, 20, 154, 26], [19, 20, 154, 26, 27], [19, 20, 154, 26, 27, 28]]\n"
     ]
    }
   ],
   "source": [
    "sequences = list()\n",
    "for line in text.split('\\n'):\n",
    "    encoded = t.texts_to_sequences([line])[0]\n",
    "    for i in range(1, len(encoded)):\n",
    "        sequence = encoded[:i+1]\n",
    "        sequences.append(sequence)\n",
    "        \n",
    "print('훈련 데이터의 개수: %d' % len(sequences))\n",
    "\n",
    "print(sequences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    }
   ],
   "source": [
    "print(max(len(l) for l in sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "sequences = pad_sequences(sequences, maxlen=6, padding='pre')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0   0   0   0   4   8]\n",
      " [  0   0   0   4   8   9]\n",
      " [  0   0   4   8   9  29]\n",
      " [  0   4   8   9  29  30]\n",
      " [  4   8   9  29  30  31]\n",
      " [  0   0   0   0  32   5]\n",
      " [  0   0   0  32   5  33]\n",
      " [  0   0  32   5  33  34]\n",
      " [  0  32   5  33  34  10]\n",
      " [ 32   5  33  34  10   6]\n",
      " [  5  33  34  10   6  35]\n",
      " [ 33  34  10   6  35   7]\n",
      " [  0   0   0   0  36  11]\n",
      " [  0   0   0  36  11  12]\n",
      " [  0   0  36  11  12  37]\n",
      " [  0  36  11  12  37  13]\n",
      " [ 36  11  12  37  13  38]\n",
      " [ 11  12  37  13  38  39]\n",
      " [ 12  37  13  38  39  40]\n",
      " [  0   0   0   0  41  42]\n",
      " [  0   0   0  41  42  43]\n",
      " [  0   0  41  42  43  44]\n",
      " [  0  41  42  43  44  45]\n",
      " [ 41  42  43  44  45  46]\n",
      " [  0   0   0   0  14  47]\n",
      " [  0   0   0  14  47  15]\n",
      " [  0   0  14  47  15  48]\n",
      " [  0  14  47  15  48  49]\n",
      " [ 14  47  15  48  49  14]\n",
      " [  0   0   0   0  50  51]\n",
      " [  0   0   0  50  51  52]\n",
      " [  0   0  50  51  52  53]\n",
      " [  0  50  51  52  53  15]\n",
      " [ 50  51  52  53  15  54]\n",
      " [  0   0   0   0  55  56]\n",
      " [  0   0   0  55  56  57]\n",
      " [  0   0  55  56  57  16]\n",
      " [  0  55  56  57  16  13]\n",
      " [ 55  56  57  16  13  58]\n",
      " [ 56  57  16  13  58  59]\n",
      " [ 57  16  13  58  59  60]\n",
      " [ 16  13  58  59  60  61]\n",
      " [  0   0   0   0  62  63]\n",
      " [  0   0   0  62  63  64]\n",
      " [  0   0  62  63  64  65]\n",
      " [  0  62  63  64  65  66]\n",
      " [ 62  63  64  65  66  67]\n",
      " [ 63  64  65  66  67  68]\n",
      " [  0   0   0   0  69  10]\n",
      " [  0   0   0  69  10  70]\n",
      " [  0   0   0   0   8  71]\n",
      " [  0   0   0   8  71  72]\n",
      " [  0   0   8  71  72  73]\n",
      " [  0   8  71  72  73  74]\n",
      " [  8  71  72  73  74  75]\n",
      " [ 71  72  73  74  75  11]\n",
      " [  0   0   0   0  12  76]\n",
      " [  0   0   0  12  76  77]\n",
      " [  0   0  12  76  77  78]\n",
      " [  0  12  76  77  78   6]\n",
      " [ 12  76  77  78   6  79]\n",
      " [ 76  77  78   6  79  80]\n",
      " [  0   0   0   0  81  82]\n",
      " [  0   0   0  81  82  83]\n",
      " [  0   0  81  82  83  84]\n",
      " [  0  81  82  83  84  17]\n",
      " [ 81  82  83  84  17  85]\n",
      " [ 82  83  84  17  85  86]\n",
      " [ 83  84  17  85  86  87]\n",
      " [  0   0   0   0  88  18]\n",
      " [  0   0   0  88  18  17]\n",
      " [  0   0  88  18  17  89]\n",
      " [  0  88  18  17  89  90]\n",
      " [ 88  18  17  89  90  91]\n",
      " [  0   0   0   0  92  93]\n",
      " [  0   0   0  92  93  94]\n",
      " [  0   0  92  93  94  95]\n",
      " [  0  92  93  94  95  96]\n",
      " [ 92  93  94  95  96  97]\n",
      " [ 93  94  95  96  97  19]\n",
      " [ 94  95  96  97  19  20]\n",
      " [  0   0   0   0  98  99]\n",
      " [  0   0   0  98  99 100]\n",
      " [  0   0  98  99 100 101]\n",
      " [  0  98  99 100 101 102]\n",
      " [ 98  99 100 101 102 103]\n",
      " [ 99 100 101 102 103 104]\n",
      " [100 101 102 103 104 105]\n",
      " [101 102 103 104 105 106]\n",
      " [102 103 104 105 106 107]\n",
      " [103 104 105 106 107 108]\n",
      " [104 105 106 107 108 109]\n",
      " [  0   0   0   0 110  21]\n",
      " [  0   0   0 110  21 111]\n",
      " [  0   0 110  21 111 112]\n",
      " [  0 110  21 111 112 113]\n",
      " [110  21 111 112 113 114]\n",
      " [  0   0   0   0   1   2]\n",
      " [  0   0   0   1   2   3]\n",
      " [  0   0   0   0   5   4]\n",
      " [  0   0   0   5   4 115]\n",
      " [  0   0   5   4 115 116]\n",
      " [  0   5   4 115 116 117]\n",
      " [  5   4 115 116 117   9]\n",
      " [  4 115 116 117   9   1]\n",
      " [115 116 117   9   1 118]\n",
      " [116 117   9   1 118   7]\n",
      " [117   9   1 118   7   2]\n",
      " [  9   1 118   7   2 119]\n",
      " [  1 118   7   2 119   7]\n",
      " [118   7   2 119   7   3]\n",
      " [  7   2 119   7   3 120]\n",
      " [  2 119   7   3 120 121]\n",
      " [119   7   3 120 121 122]\n",
      " [  7   3 120 121 122 123]\n",
      " [  3 120 121 122 123 124]\n",
      " [  0   0   0   0   1   2]\n",
      " [  0   0   0   1   2   3]\n",
      " [  0   0   0   0   5 125]\n",
      " [  0   0   0   5 125 126]\n",
      " [  0   0   5 125 126 127]\n",
      " [  0   5 125 126 127 128]\n",
      " [  5 125 126 127 128 129]\n",
      " [125 126 127 128 129   1]\n",
      " [126 127 128 129   1 130]\n",
      " [127 128 129   1 130  22]\n",
      " [128 129   1 130  22 131]\n",
      " [129   1 130  22 131   2]\n",
      " [  1 130  22 131   2 132]\n",
      " [130  22 131   2 132  22]\n",
      " [ 22 131   2 132  22 133]\n",
      " [131   2 132  22 133  23]\n",
      " [  2 132  22 133  23 134]\n",
      " [132  22 133  23 134  24]\n",
      " [ 22 133  23 134  24   3]\n",
      " [133  23 134  24   3  25]\n",
      " [ 23 134  24   3  25  23]\n",
      " [  0   0   0   0 135 136]\n",
      " [  0   0   0 135 136 137]\n",
      " [  0   0 135 136 137  24]\n",
      " [  0   0   0   0 138  26]\n",
      " [  0   0   0 138  26  27]\n",
      " [  0   0 138  26  27  28]\n",
      " [  0   0   0   0   4 139]\n",
      " [  0   0   0   4 139 140]\n",
      " [  0   0   4 139 140   6]\n",
      " [  0   4 139 140   6 141]\n",
      " [  4 139 140   6 141 142]\n",
      " [  0   0   0   0 143 144]\n",
      " [  0   0   0 143 144  16]\n",
      " [  0   0 143 144  16 145]\n",
      " [  0 143 144  16 145 146]\n",
      " [143 144  16 145 146  18]\n",
      " [144  16 145 146  18 147]\n",
      " [  0   0   0   0 148 149]\n",
      " [  0   0   0 148 149 150]\n",
      " [  0   0 148 149 150  25]\n",
      " [  0 148 149 150  25 151]\n",
      " [148 149 150  25 151 152]\n",
      " [149 150  25 151 152 153]\n",
      " [  0   0   0   0  19  20]\n",
      " [  0   0   0  19  20 154]\n",
      " [  0   0  19  20 154  26]\n",
      " [  0  19  20 154  26  27]\n",
      " [ 19  20 154  26  27  28]]\n"
     ]
    }
   ],
   "source": [
    "print(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "sequences = np.array(sequences)\n",
    "X = sequences[:, :-1]\n",
    "y = sequences[:, -1]\n",
    "# 리스트의 마지막 열을 제외하고 저장한 것은 X\n",
    "# 리스트의 마지막 열만 저장한 것은 y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0   0   0   0   4]\n",
      " [  0   0   0   4   8]\n",
      " [  0   0   4   8   9]\n",
      " [  0   4   8   9  29]\n",
      " [  4   8   9  29  30]\n",
      " [  0   0   0   0  32]\n",
      " [  0   0   0  32   5]\n",
      " [  0   0  32   5  33]\n",
      " [  0  32   5  33  34]\n",
      " [ 32   5  33  34  10]\n",
      " [  5  33  34  10   6]\n",
      " [ 33  34  10   6  35]\n",
      " [  0   0   0   0  36]\n",
      " [  0   0   0  36  11]\n",
      " [  0   0  36  11  12]\n",
      " [  0  36  11  12  37]\n",
      " [ 36  11  12  37  13]\n",
      " [ 11  12  37  13  38]\n",
      " [ 12  37  13  38  39]\n",
      " [  0   0   0   0  41]\n",
      " [  0   0   0  41  42]\n",
      " [  0   0  41  42  43]\n",
      " [  0  41  42  43  44]\n",
      " [ 41  42  43  44  45]\n",
      " [  0   0   0   0  14]\n",
      " [  0   0   0  14  47]\n",
      " [  0   0  14  47  15]\n",
      " [  0  14  47  15  48]\n",
      " [ 14  47  15  48  49]\n",
      " [  0   0   0   0  50]\n",
      " [  0   0   0  50  51]\n",
      " [  0   0  50  51  52]\n",
      " [  0  50  51  52  53]\n",
      " [ 50  51  52  53  15]\n",
      " [  0   0   0   0  55]\n",
      " [  0   0   0  55  56]\n",
      " [  0   0  55  56  57]\n",
      " [  0  55  56  57  16]\n",
      " [ 55  56  57  16  13]\n",
      " [ 56  57  16  13  58]\n",
      " [ 57  16  13  58  59]\n",
      " [ 16  13  58  59  60]\n",
      " [  0   0   0   0  62]\n",
      " [  0   0   0  62  63]\n",
      " [  0   0  62  63  64]\n",
      " [  0  62  63  64  65]\n",
      " [ 62  63  64  65  66]\n",
      " [ 63  64  65  66  67]\n",
      " [  0   0   0   0  69]\n",
      " [  0   0   0  69  10]\n",
      " [  0   0   0   0   8]\n",
      " [  0   0   0   8  71]\n",
      " [  0   0   8  71  72]\n",
      " [  0   8  71  72  73]\n",
      " [  8  71  72  73  74]\n",
      " [ 71  72  73  74  75]\n",
      " [  0   0   0   0  12]\n",
      " [  0   0   0  12  76]\n",
      " [  0   0  12  76  77]\n",
      " [  0  12  76  77  78]\n",
      " [ 12  76  77  78   6]\n",
      " [ 76  77  78   6  79]\n",
      " [  0   0   0   0  81]\n",
      " [  0   0   0  81  82]\n",
      " [  0   0  81  82  83]\n",
      " [  0  81  82  83  84]\n",
      " [ 81  82  83  84  17]\n",
      " [ 82  83  84  17  85]\n",
      " [ 83  84  17  85  86]\n",
      " [  0   0   0   0  88]\n",
      " [  0   0   0  88  18]\n",
      " [  0   0  88  18  17]\n",
      " [  0  88  18  17  89]\n",
      " [ 88  18  17  89  90]\n",
      " [  0   0   0   0  92]\n",
      " [  0   0   0  92  93]\n",
      " [  0   0  92  93  94]\n",
      " [  0  92  93  94  95]\n",
      " [ 92  93  94  95  96]\n",
      " [ 93  94  95  96  97]\n",
      " [ 94  95  96  97  19]\n",
      " [  0   0   0   0  98]\n",
      " [  0   0   0  98  99]\n",
      " [  0   0  98  99 100]\n",
      " [  0  98  99 100 101]\n",
      " [ 98  99 100 101 102]\n",
      " [ 99 100 101 102 103]\n",
      " [100 101 102 103 104]\n",
      " [101 102 103 104 105]\n",
      " [102 103 104 105 106]\n",
      " [103 104 105 106 107]\n",
      " [104 105 106 107 108]\n",
      " [  0   0   0   0 110]\n",
      " [  0   0   0 110  21]\n",
      " [  0   0 110  21 111]\n",
      " [  0 110  21 111 112]\n",
      " [110  21 111 112 113]\n",
      " [  0   0   0   0   1]\n",
      " [  0   0   0   1   2]\n",
      " [  0   0   0   0   5]\n",
      " [  0   0   0   5   4]\n",
      " [  0   0   5   4 115]\n",
      " [  0   5   4 115 116]\n",
      " [  5   4 115 116 117]\n",
      " [  4 115 116 117   9]\n",
      " [115 116 117   9   1]\n",
      " [116 117   9   1 118]\n",
      " [117   9   1 118   7]\n",
      " [  9   1 118   7   2]\n",
      " [  1 118   7   2 119]\n",
      " [118   7   2 119   7]\n",
      " [  7   2 119   7   3]\n",
      " [  2 119   7   3 120]\n",
      " [119   7   3 120 121]\n",
      " [  7   3 120 121 122]\n",
      " [  3 120 121 122 123]\n",
      " [  0   0   0   0   1]\n",
      " [  0   0   0   1   2]\n",
      " [  0   0   0   0   5]\n",
      " [  0   0   0   5 125]\n",
      " [  0   0   5 125 126]\n",
      " [  0   5 125 126 127]\n",
      " [  5 125 126 127 128]\n",
      " [125 126 127 128 129]\n",
      " [126 127 128 129   1]\n",
      " [127 128 129   1 130]\n",
      " [128 129   1 130  22]\n",
      " [129   1 130  22 131]\n",
      " [  1 130  22 131   2]\n",
      " [130  22 131   2 132]\n",
      " [ 22 131   2 132  22]\n",
      " [131   2 132  22 133]\n",
      " [  2 132  22 133  23]\n",
      " [132  22 133  23 134]\n",
      " [ 22 133  23 134  24]\n",
      " [133  23 134  24   3]\n",
      " [ 23 134  24   3  25]\n",
      " [  0   0   0   0 135]\n",
      " [  0   0   0 135 136]\n",
      " [  0   0 135 136 137]\n",
      " [  0   0   0   0 138]\n",
      " [  0   0   0 138  26]\n",
      " [  0   0 138  26  27]\n",
      " [  0   0   0   0   4]\n",
      " [  0   0   0   4 139]\n",
      " [  0   0   4 139 140]\n",
      " [  0   4 139 140   6]\n",
      " [  4 139 140   6 141]\n",
      " [  0   0   0   0 143]\n",
      " [  0   0   0 143 144]\n",
      " [  0   0 143 144  16]\n",
      " [  0 143 144  16 145]\n",
      " [143 144  16 145 146]\n",
      " [144  16 145 146  18]\n",
      " [  0   0   0   0 148]\n",
      " [  0   0   0 148 149]\n",
      " [  0   0 148 149 150]\n",
      " [  0 148 149 150  25]\n",
      " [148 149 150  25 151]\n",
      " [149 150  25 151 152]\n",
      " [  0   0   0   0  19]\n",
      " [  0   0   0  19  20]\n",
      " [  0   0  19  20 154]\n",
      " [  0  19  20 154  26]\n",
      " [ 19  20 154  26  27]]\n",
      "[  8   9  29  30  31   5  33  34  10   6  35   7  11  12  37  13  38  39\n",
      "  40  42  43  44  45  46  47  15  48  49  14  51  52  53  15  54  56  57\n",
      "  16  13  58  59  60  61  63  64  65  66  67  68  10  70  71  72  73  74\n",
      "  75  11  76  77  78   6  79  80  82  83  84  17  85  86  87  18  17  89\n",
      "  90  91  93  94  95  96  97  19  20  99 100 101 102 103 104 105 106 107\n",
      " 108 109  21 111 112 113 114   2   3   4 115 116 117   9   1 118   7   2\n",
      " 119   7   3 120 121 122 123 124   2   3 125 126 127 128 129   1 130  22\n",
      " 131   2 132  22 133  23 134  24   3  25  23 136 137  24  26  27  28 139\n",
      " 140   6 141 142 144  16 145 146  18 147 149 150  25 151 152 153  20 154\n",
      "  26  27  28]\n"
     ]
    }
   ],
   "source": [
    "print(X)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import SimpleRNN\n",
    "tf.compat.v1.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "y = to_categorical(y, num_classes=vocab_size)\n",
    "# y에 대한 원-핫 인코딩 수행\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "6/6 - 1s - loss: 5.0465 - accuracy: 0.0121\n",
      "Epoch 2/200\n",
      "6/6 - 0s - loss: 5.0337 - accuracy: 0.0121\n",
      "Epoch 3/200\n",
      "6/6 - 0s - loss: 5.0235 - accuracy: 0.0182\n",
      "Epoch 4/200\n",
      "6/6 - 0s - loss: 5.0136 - accuracy: 0.0545\n",
      "Epoch 5/200\n",
      "6/6 - 0s - loss: 5.0028 - accuracy: 0.0788\n",
      "Epoch 6/200\n",
      "6/6 - 0s - loss: 4.9906 - accuracy: 0.0970\n",
      "Epoch 7/200\n",
      "6/6 - 0s - loss: 4.9760 - accuracy: 0.1091\n",
      "Epoch 8/200\n",
      "6/6 - 0s - loss: 4.9595 - accuracy: 0.1273\n",
      "Epoch 9/200\n",
      "6/6 - 0s - loss: 4.9380 - accuracy: 0.1212\n",
      "Epoch 10/200\n",
      "6/6 - 0s - loss: 4.9135 - accuracy: 0.1394\n",
      "Epoch 11/200\n",
      "6/6 - 0s - loss: 4.8837 - accuracy: 0.1515\n",
      "Epoch 12/200\n",
      "6/6 - 0s - loss: 4.8449 - accuracy: 0.1576\n",
      "Epoch 13/200\n",
      "6/6 - 0s - loss: 4.8003 - accuracy: 0.1515\n",
      "Epoch 14/200\n",
      "6/6 - 0s - loss: 4.7469 - accuracy: 0.1636\n",
      "Epoch 15/200\n",
      "6/6 - 0s - loss: 4.6882 - accuracy: 0.1758\n",
      "Epoch 16/200\n",
      "6/6 - 0s - loss: 4.6229 - accuracy: 0.2061\n",
      "Epoch 17/200\n",
      "6/6 - 0s - loss: 4.5514 - accuracy: 0.2061\n",
      "Epoch 18/200\n",
      "6/6 - 0s - loss: 4.4774 - accuracy: 0.2121\n",
      "Epoch 19/200\n",
      "6/6 - 0s - loss: 4.4054 - accuracy: 0.2121\n",
      "Epoch 20/200\n",
      "6/6 - 0s - loss: 4.3317 - accuracy: 0.2182\n",
      "Epoch 21/200\n",
      "6/6 - 0s - loss: 4.2641 - accuracy: 0.2242\n",
      "Epoch 22/200\n",
      "6/6 - 0s - loss: 4.1924 - accuracy: 0.2667\n",
      "Epoch 23/200\n",
      "6/6 - 0s - loss: 4.1228 - accuracy: 0.3091\n",
      "Epoch 24/200\n",
      "6/6 - 0s - loss: 4.0562 - accuracy: 0.3273\n",
      "Epoch 25/200\n",
      "6/6 - 0s - loss: 3.9925 - accuracy: 0.3394\n",
      "Epoch 26/200\n",
      "6/6 - 0s - loss: 3.9282 - accuracy: 0.3697\n",
      "Epoch 27/200\n",
      "6/6 - 0s - loss: 3.8581 - accuracy: 0.4061\n",
      "Epoch 28/200\n",
      "6/6 - 0s - loss: 3.7930 - accuracy: 0.4303\n",
      "Epoch 29/200\n",
      "6/6 - 0s - loss: 3.7348 - accuracy: 0.4545\n",
      "Epoch 30/200\n",
      "6/6 - 0s - loss: 3.6725 - accuracy: 0.4727\n",
      "Epoch 31/200\n",
      "6/6 - 0s - loss: 3.6137 - accuracy: 0.4788\n",
      "Epoch 32/200\n",
      "6/6 - 0s - loss: 3.5575 - accuracy: 0.4848\n",
      "Epoch 33/200\n",
      "6/6 - 0s - loss: 3.5015 - accuracy: 0.4970\n",
      "Epoch 34/200\n",
      "6/6 - 0s - loss: 3.4477 - accuracy: 0.5030\n",
      "Epoch 35/200\n",
      "6/6 - 0s - loss: 3.4009 - accuracy: 0.4909\n",
      "Epoch 36/200\n",
      "6/6 - 0s - loss: 3.3472 - accuracy: 0.4909\n",
      "Epoch 37/200\n",
      "6/6 - 0s - loss: 3.2967 - accuracy: 0.5212\n",
      "Epoch 38/200\n",
      "6/6 - 0s - loss: 3.2471 - accuracy: 0.5394\n",
      "Epoch 39/200\n",
      "6/6 - 0s - loss: 3.1992 - accuracy: 0.5576\n",
      "Epoch 40/200\n",
      "6/6 - 0s - loss: 3.1527 - accuracy: 0.5697\n",
      "Epoch 41/200\n",
      "6/6 - 0s - loss: 3.1064 - accuracy: 0.5636\n",
      "Epoch 42/200\n",
      "6/6 - 0s - loss: 3.0617 - accuracy: 0.5697\n",
      "Epoch 43/200\n",
      "6/6 - 0s - loss: 3.0164 - accuracy: 0.6000\n",
      "Epoch 44/200\n",
      "6/6 - 0s - loss: 2.9734 - accuracy: 0.6000\n",
      "Epoch 45/200\n",
      "6/6 - 0s - loss: 2.9321 - accuracy: 0.5879\n",
      "Epoch 46/200\n",
      "6/6 - 0s - loss: 2.8926 - accuracy: 0.5879\n",
      "Epoch 47/200\n",
      "6/6 - 0s - loss: 2.8494 - accuracy: 0.6121\n",
      "Epoch 48/200\n",
      "6/6 - 0s - loss: 2.8128 - accuracy: 0.6121\n",
      "Epoch 49/200\n",
      "6/6 - 0s - loss: 2.7710 - accuracy: 0.6242\n",
      "Epoch 50/200\n",
      "6/6 - 0s - loss: 2.7338 - accuracy: 0.6485\n",
      "Epoch 51/200\n",
      "6/6 - 0s - loss: 2.6943 - accuracy: 0.6606\n",
      "Epoch 52/200\n",
      "6/6 - 0s - loss: 2.6554 - accuracy: 0.6606\n",
      "Epoch 53/200\n",
      "6/6 - 0s - loss: 2.6203 - accuracy: 0.6667\n",
      "Epoch 54/200\n",
      "6/6 - 0s - loss: 2.5872 - accuracy: 0.6667\n",
      "Epoch 55/200\n",
      "6/6 - 0s - loss: 2.5477 - accuracy: 0.6788\n",
      "Epoch 56/200\n",
      "6/6 - 0s - loss: 2.5142 - accuracy: 0.6909\n",
      "Epoch 57/200\n",
      "6/6 - 0s - loss: 2.4792 - accuracy: 0.7091\n",
      "Epoch 58/200\n",
      "6/6 - 0s - loss: 2.4480 - accuracy: 0.6727\n",
      "Epoch 59/200\n",
      "6/6 - 0s - loss: 2.4132 - accuracy: 0.7091\n",
      "Epoch 60/200\n",
      "6/6 - 0s - loss: 2.3837 - accuracy: 0.7333\n",
      "Epoch 61/200\n",
      "6/6 - 0s - loss: 2.3493 - accuracy: 0.7152\n",
      "Epoch 62/200\n",
      "6/6 - 0s - loss: 2.3195 - accuracy: 0.7152\n",
      "Epoch 63/200\n",
      "6/6 - 0s - loss: 2.2898 - accuracy: 0.7212\n",
      "Epoch 64/200\n",
      "6/6 - 0s - loss: 2.2573 - accuracy: 0.7333\n",
      "Epoch 65/200\n",
      "6/6 - 0s - loss: 2.2291 - accuracy: 0.7152\n",
      "Epoch 66/200\n",
      "6/6 - 0s - loss: 2.2006 - accuracy: 0.7212\n",
      "Epoch 67/200\n",
      "6/6 - 0s - loss: 2.1672 - accuracy: 0.7576\n",
      "Epoch 68/200\n",
      "6/6 - 0s - loss: 2.1378 - accuracy: 0.7455\n",
      "Epoch 69/200\n",
      "6/6 - 0s - loss: 2.1104 - accuracy: 0.7515\n",
      "Epoch 70/200\n",
      "6/6 - 0s - loss: 2.0815 - accuracy: 0.7818\n",
      "Epoch 71/200\n",
      "6/6 - 0s - loss: 2.0628 - accuracy: 0.7576\n",
      "Epoch 72/200\n",
      "6/6 - 0s - loss: 2.0290 - accuracy: 0.7697\n",
      "Epoch 73/200\n",
      "6/6 - 0s - loss: 2.0026 - accuracy: 0.7697\n",
      "Epoch 74/200\n",
      "6/6 - 0s - loss: 1.9813 - accuracy: 0.7758\n",
      "Epoch 75/200\n",
      "6/6 - 0s - loss: 1.9562 - accuracy: 0.7818\n",
      "Epoch 76/200\n",
      "6/6 - 0s - loss: 1.9215 - accuracy: 0.7636\n",
      "Epoch 77/200\n",
      "6/6 - 0s - loss: 1.8976 - accuracy: 0.7879\n",
      "Epoch 78/200\n",
      "6/6 - 0s - loss: 1.8760 - accuracy: 0.7879\n",
      "Epoch 79/200\n",
      "6/6 - 0s - loss: 1.8566 - accuracy: 0.7697\n",
      "Epoch 80/200\n",
      "6/6 - 0s - loss: 1.8345 - accuracy: 0.7758\n",
      "Epoch 81/200\n",
      "6/6 - 0s - loss: 1.8022 - accuracy: 0.8061\n",
      "Epoch 82/200\n",
      "6/6 - 0s - loss: 1.7807 - accuracy: 0.8242\n",
      "Epoch 83/200\n",
      "6/6 - 0s - loss: 1.7592 - accuracy: 0.8121\n",
      "Epoch 84/200\n",
      "6/6 - 0s - loss: 1.7368 - accuracy: 0.8121\n",
      "Epoch 85/200\n",
      "6/6 - 0s - loss: 1.7128 - accuracy: 0.8364\n",
      "Epoch 86/200\n",
      "6/6 - 0s - loss: 1.7001 - accuracy: 0.8121\n",
      "Epoch 87/200\n",
      "6/6 - 0s - loss: 1.6743 - accuracy: 0.8182\n",
      "Epoch 88/200\n",
      "6/6 - 0s - loss: 1.6537 - accuracy: 0.8242\n",
      "Epoch 89/200\n",
      "6/6 - 0s - loss: 1.6307 - accuracy: 0.8364\n",
      "Epoch 90/200\n",
      "6/6 - 0s - loss: 1.6080 - accuracy: 0.8424\n",
      "Epoch 91/200\n",
      "6/6 - 0s - loss: 1.5893 - accuracy: 0.8485\n",
      "Epoch 92/200\n",
      "6/6 - 0s - loss: 1.5780 - accuracy: 0.8121\n",
      "Epoch 93/200\n",
      "6/6 - 0s - loss: 1.5506 - accuracy: 0.8606\n",
      "Epoch 94/200\n",
      "6/6 - 0s - loss: 1.5373 - accuracy: 0.8545\n",
      "Epoch 95/200\n",
      "6/6 - 0s - loss: 1.5166 - accuracy: 0.8364\n",
      "Epoch 96/200\n",
      "6/6 - 0s - loss: 1.5012 - accuracy: 0.8545\n",
      "Epoch 97/200\n",
      "6/6 - 0s - loss: 1.4753 - accuracy: 0.8667\n",
      "Epoch 98/200\n",
      "6/6 - 0s - loss: 1.4715 - accuracy: 0.8242\n",
      "Epoch 99/200\n",
      "6/6 - 0s - loss: 1.4451 - accuracy: 0.8667\n",
      "Epoch 100/200\n",
      "6/6 - 0s - loss: 1.4251 - accuracy: 0.8606\n",
      "Epoch 101/200\n",
      "6/6 - 0s - loss: 1.4110 - accuracy: 0.8606\n",
      "Epoch 102/200\n",
      "6/6 - 0s - loss: 1.3944 - accuracy: 0.8242\n",
      "Epoch 103/200\n",
      "6/6 - 0s - loss: 1.3751 - accuracy: 0.8485\n",
      "Epoch 104/200\n",
      "6/6 - 0s - loss: 1.3602 - accuracy: 0.8667\n",
      "Epoch 105/200\n",
      "6/6 - 0s - loss: 1.3555 - accuracy: 0.8545\n",
      "Epoch 106/200\n",
      "6/6 - 0s - loss: 1.3239 - accuracy: 0.8606\n",
      "Epoch 107/200\n",
      "6/6 - 0s - loss: 1.3116 - accuracy: 0.8606\n",
      "Epoch 108/200\n",
      "6/6 - 0s - loss: 1.2924 - accuracy: 0.8848\n",
      "Epoch 109/200\n",
      "6/6 - 0s - loss: 1.2799 - accuracy: 0.8788\n",
      "Epoch 110/200\n",
      "6/6 - 0s - loss: 1.2626 - accuracy: 0.8788\n",
      "Epoch 111/200\n",
      "6/6 - 0s - loss: 1.2491 - accuracy: 0.8909\n",
      "Epoch 112/200\n",
      "6/6 - 0s - loss: 1.2316 - accuracy: 0.8909\n",
      "Epoch 113/200\n",
      "6/6 - 0s - loss: 1.2321 - accuracy: 0.8727\n",
      "Epoch 114/200\n",
      "6/6 - 0s - loss: 1.2130 - accuracy: 0.8727\n",
      "Epoch 115/200\n",
      "6/6 - 0s - loss: 1.1935 - accuracy: 0.8788\n",
      "Epoch 116/200\n",
      "6/6 - 0s - loss: 1.1851 - accuracy: 0.8788\n",
      "Epoch 117/200\n",
      "6/6 - 0s - loss: 1.1756 - accuracy: 0.8727\n",
      "Epoch 118/200\n",
      "6/6 - 0s - loss: 1.1668 - accuracy: 0.8909\n",
      "Epoch 119/200\n",
      "6/6 - 0s - loss: 1.1365 - accuracy: 0.8970\n",
      "Epoch 120/200\n",
      "6/6 - 0s - loss: 1.1261 - accuracy: 0.8970\n",
      "Epoch 121/200\n",
      "6/6 - 0s - loss: 1.1123 - accuracy: 0.8970\n",
      "Epoch 122/200\n",
      "6/6 - 0s - loss: 1.0995 - accuracy: 0.9152\n",
      "Epoch 123/200\n",
      "6/6 - 0s - loss: 1.0846 - accuracy: 0.9030\n",
      "Epoch 124/200\n",
      "6/6 - 0s - loss: 1.0716 - accuracy: 0.9030\n",
      "Epoch 125/200\n",
      "6/6 - 0s - loss: 1.0590 - accuracy: 0.9030\n",
      "Epoch 126/200\n",
      "6/6 - 0s - loss: 1.0493 - accuracy: 0.9091\n",
      "Epoch 127/200\n",
      "6/6 - 0s - loss: 1.0508 - accuracy: 0.9091\n",
      "Epoch 128/200\n",
      "6/6 - 0s - loss: 1.0273 - accuracy: 0.8848\n",
      "Epoch 129/200\n",
      "6/6 - 0s - loss: 1.0134 - accuracy: 0.9152\n",
      "Epoch 130/200\n",
      "6/6 - 0s - loss: 1.0048 - accuracy: 0.9212\n",
      "Epoch 131/200\n",
      "6/6 - 0s - loss: 0.9911 - accuracy: 0.9091\n",
      "Epoch 132/200\n",
      "6/6 - 0s - loss: 0.9835 - accuracy: 0.8909\n",
      "Epoch 133/200\n",
      "6/6 - 0s - loss: 0.9799 - accuracy: 0.9030\n",
      "Epoch 134/200\n",
      "6/6 - 0s - loss: 0.9542 - accuracy: 0.9515\n",
      "Epoch 135/200\n",
      "6/6 - 0s - loss: 0.9421 - accuracy: 0.9394\n",
      "Epoch 136/200\n",
      "6/6 - 0s - loss: 0.9357 - accuracy: 0.9212\n",
      "Epoch 137/200\n",
      "6/6 - 0s - loss: 0.9221 - accuracy: 0.9212\n",
      "Epoch 138/200\n",
      "6/6 - 0s - loss: 0.9060 - accuracy: 0.9394\n",
      "Epoch 139/200\n",
      "6/6 - 0s - loss: 0.8993 - accuracy: 0.9333\n",
      "Epoch 140/200\n",
      "6/6 - 0s - loss: 0.8841 - accuracy: 0.9576\n",
      "Epoch 141/200\n",
      "6/6 - 0s - loss: 0.8796 - accuracy: 0.9333\n",
      "Epoch 142/200\n",
      "6/6 - 0s - loss: 0.8765 - accuracy: 0.9212\n",
      "Epoch 143/200\n",
      "6/6 - 0s - loss: 0.8561 - accuracy: 0.9394\n",
      "Epoch 144/200\n",
      "6/6 - 0s - loss: 0.8480 - accuracy: 0.9515\n",
      "Epoch 145/200\n",
      "6/6 - 0s - loss: 0.8348 - accuracy: 0.9515\n",
      "Epoch 146/200\n",
      "6/6 - 0s - loss: 0.8246 - accuracy: 0.9455\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 147/200\n",
      "6/6 - 0s - loss: 0.8174 - accuracy: 0.9455\n",
      "Epoch 148/200\n",
      "6/6 - 0s - loss: 0.8130 - accuracy: 0.9394\n",
      "Epoch 149/200\n",
      "6/6 - 0s - loss: 0.8003 - accuracy: 0.9455\n",
      "Epoch 150/200\n",
      "6/6 - 0s - loss: 0.7901 - accuracy: 0.9455\n",
      "Epoch 151/200\n",
      "6/6 - 0s - loss: 0.7824 - accuracy: 0.9394\n",
      "Epoch 152/200\n",
      "6/6 - 0s - loss: 0.7733 - accuracy: 0.9394\n",
      "Epoch 153/200\n",
      "6/6 - 0s - loss: 0.7776 - accuracy: 0.9576\n",
      "Epoch 154/200\n",
      "6/6 - 0s - loss: 0.7722 - accuracy: 0.9758\n",
      "Epoch 155/200\n",
      "6/6 - 0s - loss: 0.7529 - accuracy: 0.9576\n",
      "Epoch 156/200\n",
      "6/6 - 0s - loss: 0.7456 - accuracy: 0.9394\n",
      "Epoch 157/200\n",
      "6/6 - 0s - loss: 0.7276 - accuracy: 0.9515\n",
      "Epoch 158/200\n",
      "6/6 - 0s - loss: 0.7177 - accuracy: 0.9697\n",
      "Epoch 159/200\n",
      "6/6 - 0s - loss: 0.7093 - accuracy: 0.9636\n",
      "Epoch 160/200\n",
      "6/6 - 0s - loss: 0.7005 - accuracy: 0.9576\n",
      "Epoch 161/200\n",
      "6/6 - 0s - loss: 0.6941 - accuracy: 0.9515\n",
      "Epoch 162/200\n",
      "6/6 - 0s - loss: 0.6842 - accuracy: 0.9636\n",
      "Epoch 163/200\n",
      "6/6 - 0s - loss: 0.6805 - accuracy: 0.9818\n",
      "Epoch 164/200\n",
      "6/6 - 0s - loss: 0.6710 - accuracy: 0.9697\n",
      "Epoch 165/200\n",
      "6/6 - 0s - loss: 0.6694 - accuracy: 0.9576\n",
      "Epoch 166/200\n",
      "6/6 - 0s - loss: 0.6570 - accuracy: 0.9758\n",
      "Epoch 167/200\n",
      "6/6 - 0s - loss: 0.6506 - accuracy: 0.9818\n",
      "Epoch 168/200\n",
      "6/6 - 0s - loss: 0.6425 - accuracy: 0.9758\n",
      "Epoch 169/200\n",
      "6/6 - 0s - loss: 0.6374 - accuracy: 0.9758\n",
      "Epoch 170/200\n",
      "6/6 - 0s - loss: 0.6292 - accuracy: 0.9758\n",
      "Epoch 171/200\n",
      "6/6 - 0s - loss: 0.6224 - accuracy: 0.9697\n",
      "Epoch 172/200\n",
      "6/6 - 0s - loss: 0.6159 - accuracy: 0.9758\n",
      "Epoch 173/200\n",
      "6/6 - 0s - loss: 0.6130 - accuracy: 0.9636\n",
      "Epoch 174/200\n",
      "6/6 - 0s - loss: 0.6028 - accuracy: 0.9818\n",
      "Epoch 175/200\n",
      "6/6 - 0s - loss: 0.5981 - accuracy: 0.9818\n",
      "Epoch 176/200\n",
      "6/6 - 0s - loss: 0.6000 - accuracy: 0.9455\n",
      "Epoch 177/200\n",
      "6/6 - 0s - loss: 0.5900 - accuracy: 0.9515\n",
      "Epoch 178/200\n",
      "6/6 - 0s - loss: 0.5828 - accuracy: 0.9636\n",
      "Epoch 179/200\n",
      "6/6 - 0s - loss: 0.5751 - accuracy: 0.9697\n",
      "Epoch 180/200\n",
      "6/6 - 0s - loss: 0.5742 - accuracy: 0.9515\n",
      "Epoch 181/200\n",
      "6/6 - 0s - loss: 0.5702 - accuracy: 0.9515\n",
      "Epoch 182/200\n",
      "6/6 - 0s - loss: 0.5502 - accuracy: 0.9758\n",
      "Epoch 183/200\n",
      "6/6 - 0s - loss: 0.5433 - accuracy: 0.9697\n",
      "Epoch 184/200\n",
      "6/6 - 0s - loss: 0.5428 - accuracy: 0.9697\n",
      "Epoch 185/200\n",
      "6/6 - 0s - loss: 0.5392 - accuracy: 0.9515\n",
      "Epoch 186/200\n",
      "6/6 - 0s - loss: 0.5282 - accuracy: 0.9697\n",
      "Epoch 187/200\n",
      "6/6 - 0s - loss: 0.5238 - accuracy: 0.9818\n",
      "Epoch 188/200\n",
      "6/6 - 0s - loss: 0.5156 - accuracy: 0.9697\n",
      "Epoch 189/200\n",
      "6/6 - 0s - loss: 0.5132 - accuracy: 0.9697\n",
      "Epoch 190/200\n",
      "6/6 - 0s - loss: 0.5115 - accuracy: 0.9758\n",
      "Epoch 191/200\n",
      "6/6 - 0s - loss: 0.5010 - accuracy: 0.9758\n",
      "Epoch 192/200\n",
      "6/6 - 0s - loss: 0.5016 - accuracy: 0.9758\n",
      "Epoch 193/200\n",
      "6/6 - 0s - loss: 0.4922 - accuracy: 0.9697\n",
      "Epoch 194/200\n",
      "6/6 - 0s - loss: 0.4927 - accuracy: 0.9636\n",
      "Epoch 195/200\n",
      "6/6 - 0s - loss: 0.4837 - accuracy: 0.9758\n",
      "Epoch 196/200\n",
      "6/6 - 0s - loss: 0.4717 - accuracy: 0.9818\n",
      "Epoch 197/200\n",
      "6/6 - 0s - loss: 0.4694 - accuracy: 0.9818\n",
      "Epoch 198/200\n",
      "6/6 - 0s - loss: 0.4721 - accuracy: 0.9818\n",
      "Epoch 199/200\n",
      "6/6 - 0s - loss: 0.4624 - accuracy: 0.9758\n",
      "Epoch 200/200\n",
      "6/6 - 0s - loss: 0.4622 - accuracy: 0.9636\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x19b05565f70>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.layers import Embedding, Dense, SimpleRNN\n",
    "from keras.models import Sequential\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 10, input_length=5))\n",
    "# y를 제거하였으므로 이제 x의 길이는 5\n",
    "model.add(SimpleRNN(32))\n",
    "model.add(Dense(vocab_size, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam',\n",
    "             metrics=['accuracy'])\n",
    "model.fit(X, y, epochs=200, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_generation(model, t, current_word, n):\n",
    "    # 모델, 토크나이저, 현재 단어, 반복 횟수\n",
    "    init_word = current_word\n",
    "    # 처음 들어온 단어도 마지막에 출력하기 위해 저장\n",
    "    sentence = ''\n",
    "    for _ in range(n):\n",
    "        encoded = t.texts_to_sequences([current_word])[0]\n",
    "        # 현재 단어에 대한 정수 인코딩\n",
    "        encoded = pad_sequences([encoded], maxlen=5, padding='pre')\n",
    "        # 데이터에 대한 패딩\n",
    "        result = model.predict_classes(encoded, verbose=0)\n",
    "        # 입력한 X(현재 단어)에 대해서 Y를 입력하고\n",
    "        # Y(예측한 단어)를 result에 저장.\n",
    "        for word, index in t.word_index.items():\n",
    "            if index == result:\n",
    "                break\n",
    "        current_word = current_word + ' ' + word\n",
    "        # 현재 단어 + ' ' + 예측 단어를 현재 단어로 변경\n",
    "        sentence = sentence + ' ' + word\n",
    "    # 전체 반복\n",
    "    sentence = init_word + sentence\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sentence_generation' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-6d88d9e27baf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence_generation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'특성'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m6\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'sentence_generation' is not defined"
     ]
    }
   ],
   "source": [
    "print(sentence_generation(model, t, '특성', 6))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
