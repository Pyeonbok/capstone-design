{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 형태소 분석 및 word2vec\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Twitter\n",
    "from gensim.models import Word2Vec\n",
    "import csv\n",
    " \n",
    "twitter = Twitter() # 형태소분류 클래스 선언/생성\n",
    " \n",
    "file = open(\"Article_shuffled.csv\", 'r', encoding='euc-kr')  # open file\n",
    "line = csv.reader(file)   # read csv file\n",
    "token = []\n",
    "embeddingmodel = []\n",
    " \n",
    "    \n",
    "# 토큰 처리를 위한 작업 수행\n",
    "for i in line:\n",
    "    sentence = twitter.pos(i[0], norm=True, stem=True) # 형태소분석.\n",
    "    temp = []\n",
    "    temp_embedding = []\n",
    "    all_temp = []\n",
    "    \n",
    "    for k in range(len(sentence)):\n",
    "        temp_embedding.append(sentence[k][0]) # 임베딩된 단어 추가\n",
    "        temp.append(sentence[k][0] + '/' + sentence[k][1]) # ( 단어 / 형태소 )\n",
    "    all_temp.append(temp) # 모든 (단어/형태소 ) 저장\n",
    "    embeddingmodel.append(temp_embedding) # 임베딩할 단어들 추가\n",
    "    \n",
    "    category_number_dic = {'IT과학': 0, '경제': 1, '정치': 2, 'e스포츠': 3, '골프': 4,\n",
    "   '농구': 5, '배구': 6, '야구': 7, '일반 스포츠': 8, '축구': 9, '사회': 10, '생활문화': 11}\n",
    "    # 카테고리 레이블 작성\n",
    "    all_temp.append(category_number_dic.get(category))\n",
    "    token.append(all_temp)\n",
    "    \n",
    "print(\"토큰 처리 완료\")\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec은 '비슷한 위치에서 등장하는 단어들은 비슷한 의미를 가진다'(분산가정) 라는 가정을 가진다라는 '분산표현' 방법을 사용합니다. \n",
    "- 예를 들어, 강아지는 귀엽다. 강아지는 예쁘다. 강아지는 사랑스럽다. 라는 문장 3개가 오면 귀엽다, 예쁘다, 사랑스럽다를 비슷한 의미를 가진다고 판단하는 방법입니다.  \n",
    "\n",
    "\n",
    "### 단어를 밀집 벡터(dense vector)의 형태로 표현하는 방법을 워드 임베딩(word embedding)이라고 합니다. \n",
    "- 그리고 이 밀집 벡터를 워드 임베딩 과정을 통해 나온 결과라고 하여 임베딩 벡터(embedding vector)라고도 합니다.\n",
    "- 원-핫 인코딩 챕터에서 원-핫 벡터는 단어 간 유사도를 계산할 수 없다는 단점이 있음을 언급한 적이 있습니다. 그래서 단어 간 유사도를 반영할 수 있도록 단어의 의미를 벡터화 할 수 있는 방법이 필요합니다. 그리고 이를 위해서 사용되는 대표적인 방법이 워드투벡터(Word2Vec)입니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddingmodel = []\n",
    "for i in range(len(token)):\n",
    "    temp_embeddingmodel = []\n",
    "    for k in range(len(token[i][0])):\n",
    "        temp_embeddingmodel.append(token[i][0][k])\n",
    "    embeddingmodel.append(temp_embeddingmodel)\n",
    "# max_vocab size 10000000 개당 1 GB 메모리 차지\n",
    "embedding = Word2Vec(embeddingmodel, size=300, window=5, min_count=10, iter=5, sg=1, max_vocab_size = 360000000)\n",
    "embedding.save('post.embedding')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
